from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field
from typing import Dict, Any
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Handle imports for both direct execution and module import
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent.parent))

class EmailComparisonResult(BaseModel):
    """Result of email comparison by AI judge."""
    accuracy_score: int = Field(..., description="Score 1-10 for content accuracy")
    tone_score: int = Field(..., description="Score 1-10 for dispatcher tone")
    compliance_score: int = Field(..., description="Score 1-10 for rule compliance")
    overall_score: int = Field(..., description="Overall score 1-10")
    passes: bool = Field(..., description="Whether email passes evaluation")
    feedback: str = Field(..., description="Detailed feedback on the comparison")

EMAIL_JUDGE_PROMPT = """
You are an expert evaluator for trucking dispatcher emails. Compare the EXPECTED email with the ACTUAL email generated by the AI system.

EVALUATION CRITERIA:

1. CONTENT ACCURACY (1-10):
   - Does the actual email ask for the same missing information as expected?
   - Are the field requests appropriate and complete?
   - Does it handle rates correctly (confirm if provided, ask if missing)?

2. TONE & STYLE (1-10):
   - Does it sound like a real dispatcher (casual, professional, industry terms)?
   - Uses trucking abbreviations appropriately (PU, DEL, RC, etc.)?
   - Avoids AI-like formality or politeness?
   - Direct and concise (1-2 sentences)?

3. RULE COMPLIANCE (1-10):
   - No greetings, signatures, or pleasantries?
   - No commitments or promises?
   - No unnecessary information?
   - Follows dispatcher communication patterns?

SCORING:
- 8-10: Excellent, meets or exceeds expectations
- 6-7: Good, minor issues
- 4-5: Acceptable, some problems
- 1-3: Poor, major issues

PASS THRESHOLD: Overall score >= 7

Edge Cases:
    - If the ai generated email is empty, and brokers email also empty, score it as 10.

Provide detailed feedback explaining scores and any differences.
"""

class EmailJudge:
    """AI-powered email comparison judge."""

    def __init__(self):
        self.llm = init_chat_model(
            "azure_openai:gpt-4o",
            temperature=0.3
        ).with_structured_output(EmailComparisonResult, method="function_calling")

    def compare_emails(self, expected: str, actual: str, context: Dict[str, Any] = None) -> EmailComparisonResult:
        """
        Compare expected vs actual email using AI judge.

        Args:
            expected: Expected email content
            actual: Actual generated email content
            context: Additional context about the test case

        Returns:
            EmailComparisonResult with scores and feedback
        """
        context_info = ""
        if context:
            context_info = f"TEST CONTEXT: {context.get('description', '')}\nCATEGORY: {context.get('category', '')}\n\n"

        comparison_text = f"""
{context_info}EXPECTED EMAIL:
"{expected}"

ACTUAL EMAIL:
"{actual}"

Compare these emails and evaluate the actual email's quality."""

        try:
            result = self.llm.invoke([
                SystemMessage(content=EMAIL_JUDGE_PROMPT),
                HumanMessage(content=comparison_text)
            ])
            return result
        except Exception as e:
            # Fallback evaluation if LLM fails
            return EmailComparisonResult(
                accuracy_score=1,
                tone_score=1,
                compliance_score=1,
                overall_score=1,
                passes=False,
                feedback=f"Error during evaluation: {str(e)}"
            )
